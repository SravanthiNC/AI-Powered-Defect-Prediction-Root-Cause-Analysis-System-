{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74a7ebe",
   "metadata": {},
   "source": [
    "# Manufacturing Quality Prediction Model Building\n",
    "\n",
    "This notebook focuses on building and training models for:\n",
    "1. Failure Prediction (Classification)\n",
    "2. Root Cause Analysis (Error Pattern Detection)\n",
    "3. Time Series Analysis (Sequential Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962d550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45b0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20676\\118144489.py:8: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20676\\118144489.py:8: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20676\\118144489.py:8: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20676\\118144489.py:8: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Main unit data shape: (1000, 14)\n",
      "Component data shape: (1000, 15)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "main_unit_df = pd.read_csv('main_unit_assembly_data.csv')\n",
    "component_df = pd.read_csv('component_assembly_data.csv')\n",
    "\n",
    "# Convert datetime columns\n",
    "for df in [main_unit_df, component_df]:\n",
    "    for col in ['TRNDATE', 'INSERTTIME']:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Main unit data shape: {main_unit_df.shape}\")\n",
    "print(f\"Component data shape: {component_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea895526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def create_features(df):\n",
    "    # Temporal features\n",
    "    df['hour'] = df['TRNDATE'].dt.hour\n",
    "    df['day'] = df['TRNDATE'].dt.day\n",
    "    df['month'] = df['TRNDATE'].dt.month\n",
    "    df['day_of_week'] = df['TRNDATE'].dt.dayofweek\n",
    "    \n",
    "    # Time between operations\n",
    "    df['time_diff'] = (df['TRNDATE'] - df['INSERTTIME']).dt.total_seconds()\n",
    "    \n",
    "    # Categorical encoding\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = ['LINE', 'WORKSTATION', 'STAGE', 'VENDOR']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "    \n",
    "    # Error code features\n",
    "    if 'A_ERRORCODE' in df.columns:\n",
    "        df['has_error'] = df['A_ERRORCODE'].notna().astype(int)\n",
    "    \n",
    "    # Pass/Fail encoding\n",
    "    df['target'] = (df['RESULTFLAG'] == 'F').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features for both datasets\n",
    "main_unit_df = create_features(main_unit_df)\n",
    "component_df = create_features(component_df)\n",
    "\n",
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'hour', 'day', 'month', 'day_of_week', 'time_diff',\n",
    "    'LINE_encoded', 'WORKSTATION_encoded', 'STAGE_encoded', 'VENDOR_encoded',\n",
    "    'has_error'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = main_unit_df[feature_cols]\n",
    "y = main_unit_df['target']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822908d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters:\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       181\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.45      0.50      0.48       200\n",
      "weighted avg       0.82      0.91      0.86       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srava\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "C:\\Users\\srava\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "C:\\Users\\srava\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m rf_importance = pd.DataFrame({\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m: feature_cols,\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: rf_grid.best_estimator_.feature_importances_\n\u001b[32m     26\u001b[39m }).sort_values(\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     28\u001b[39m fig = px.bar(rf_importance, x=\u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m, y=\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     29\u001b[39m              title=\u001b[33m'\u001b[39m\u001b[33mRandom Forest Feature Importance\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotly\\basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotly\\io\\_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# 1. Random Forest Model\n",
    "# Hyperparameter tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_model, rf_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest Parameters:\")\n",
    "print(rf_grid.best_params_)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_pred = rf_grid.predict(X_test)\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "# Feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_grid.best_estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig = px.bar(rf_importance, x='feature', y='importance',\n",
    "             title='Random Forest Feature Importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d234dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XGBoost Model\n",
    "# Hyperparameter tuning\n",
    "xgb_params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_pred = xgb_grid.predict(X_test)\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LSTM Model for Sequential Patterns\n",
    "def prepare_sequences(X, y, timesteps=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_seq.append(X[i:(i + timesteps)])\n",
    "        y_seq.append(y[i + timesteps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Prepare sequential data\n",
    "X_seq_train, y_seq_train = prepare_sequences(X_train, y_train)\n",
    "X_seq_test, y_seq_test = prepare_sequences(X_test, y_test)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_seq_train.shape[1], X_seq_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM\n",
    "history = lstm_model.fit(\n",
    "    X_seq_train, y_seq_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_pred = (lstm_model.predict(X_seq_test) > 0.5).astype(int)\n",
    "print(\"\\nLSTM Performance:\")\n",
    "print(classification_report(y_seq_test, lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Visualization\n",
    "def plot_model_comparison():\n",
    "    models = ['Random Forest', 'XGBoost', 'LSTM']\n",
    "    predictions = [rf_pred, xgb_pred, lstm_pred.flatten()]\n",
    "    y_tests = [y_test, y_test, y_seq_test]\n",
    "    \n",
    "    metrics = []\n",
    "    for model, pred, y_true in zip(models, predictions, y_tests):\n",
    "        metrics.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': accuracy_score(y_true, pred),\n",
    "            'F1-Score': f1_score(y_true, pred),\n",
    "            'ROC-AUC': roc_auc_score(y_true, pred)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3,\n",
    "                        subplot_titles=('Accuracy', 'F1-Score', 'ROC-AUC'))\n",
    "    \n",
    "    for i, metric in enumerate(['Accuracy', 'F1-Score', 'ROC-AUC']):\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=metrics_df['Model'],\n",
    "                   y=metrics_df[metric],\n",
    "                   name=metric),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, width=1200,\n",
    "                      title_text=\"Model Performance Comparison\")\n",
    "    fig.show()\n",
    "\n",
    "plot_model_comparison()\n",
    "\n",
    "# Save the best performing model\n",
    "import joblib\n",
    "\n",
    "# Save models and scaler\n",
    "joblib.dump(rf_grid.best_estimator_, 'rf_model.joblib')\n",
    "joblib.dump(xgb_grid.best_estimator_, 'xgb_model.joblib')\n",
    "lstm_model.save('lstm_model.h5')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# Save feature importance for deployment\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_grid.best_estimator_.feature_importances_\n",
    "})\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cdcda2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and features saved successfully!\n",
      "Saved files:\n",
      "1. manufacturing_quality_model.joblib\n",
      "2. feature_scaler.joblib\n",
      "3. feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "# Save models and scaler for dashboard\n",
    "import joblib\n",
    "\n",
    "# Save Random Forest model\n",
    "joblib.dump(rf_grid.best_estimator_, 'manufacturing_quality_model.joblib')\n",
    "joblib.dump(scaler, 'feature_scaler.joblib')\n",
    "\n",
    "# Save feature columns for reference\n",
    "import json\n",
    "with open('feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "\n",
    "print(\"Models and features saved successfully!\")\n",
    "print(\"Saved files:\")\n",
    "print(\"1. manufacturing_quality_model.joblib\")\n",
    "print(\"2. feature_scaler.joblib\")\n",
    "print(\"3. feature_columns.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
