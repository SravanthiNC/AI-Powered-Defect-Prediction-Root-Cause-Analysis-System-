{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee841b0",
   "metadata": {},
   "source": [
    "# Generate Synthetic Manufacturing Data\n",
    "\n",
    "This notebook generates two synthetic datasets:\n",
    "1. Main Unit Assembly Data\n",
    "2. Component Assembly Line Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd542262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c16129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helper functions\n",
    "def generate_datetime(start_date, num_days):\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    days = pd.date_range(start=start, periods=num_days).tolist()\n",
    "    times = [f\"{random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}\" for _ in range(num_days)]\n",
    "    return [f\"{d.strftime('%y-%m-%d')} {t}\" for d, t in zip(days, times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3c8da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_unit_data(num_rows):\n",
    "    data = {\n",
    "        'USN': [f'USN{str(i+1).zfill(6)}' for i in range(num_rows)],\n",
    "        'LINE': [f'LINE{str(random.randint(1,5)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'WORKSTATION': [f'WORKSTATION{str(random.randint(1,10)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'STAGE': [f'M{random.randint(1,5)}' for _ in range(num_rows)],\n",
    "        'TRNDATA': [f'USN{str(random.randint(1,1000)).zfill(6)}' for _ in range(num_rows)],\n",
    "        'TRNDATE': generate_datetime('2025-08-24', num_rows),\n",
    "        'PASSCOUNT': [random.randint(0,3) for _ in range(num_rows)],\n",
    "        'RESULTFLAG': [random.choices(['T', 'F'], weights=[0.9, 0.1])[0] for _ in range(num_rows)],\n",
    "        'USERID': [f'USER{str(random.randint(1,20)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'INSERTTIME': generate_datetime('2025-08-24', num_rows),\n",
    "        'LINENO': [str(random.randint(1,5)) for _ in range(num_rows)],\n",
    "        'VENDOR': [f'VENDOR{random.randint(1,5)}' for _ in range(num_rows)],\n",
    "        'A_ERRORCODE': [f'K{str(random.randint(0,9999999)).zfill(7)}' if random.random() < 0.1 else '' for _ in range(num_rows)],\n",
    "        'AQM_ERRORCODE': [f'ERROR{random.randint(1,5)}' if random.random() < 0.1 else '' for _ in range(num_rows)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "381617d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_component_data(num_rows):\n",
    "    stages = ['BTL:Double Lock Sim Data', 'BTN Conn', 'DK:Paste MLB tape and ety', 'ISO to HSG']\n",
    "    data = {\n",
    "        'USN_PRIMARY': [f'USN{str(i+1).zfill(6)}' for i in range(num_rows)],\n",
    "        'SUB_USN': [f'CSN{str(random.randint(1,999999)).zfill(6)}' for _ in range(num_rows)],\n",
    "        'LINE': [f'LINE{str(random.randint(1,5)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'WORKSTATION': [f'WORKSTATION{str(random.randint(1,10)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'STAGE': [f'C{random.randint(1,5)}' for _ in range(num_rows)],\n",
    "        'TRNDATA': [f'CSN{str(random.randint(1,1000)).zfill(6)}' for _ in range(num_rows)],\n",
    "        'TRNDATE': generate_datetime('2025-08-24', num_rows),\n",
    "        'PASSCOUNT': [random.randint(0,3) for _ in range(num_rows)],\n",
    "        'RESULTFLAG': [random.choices(['T', 'F'], weights=[0.9, 0.1])[0] for _ in range(num_rows)],\n",
    "        'USERID': [f'USER{str(random.randint(1,20)).zfill(3)}' for _ in range(num_rows)],\n",
    "        'INSERTTIME': generate_datetime('2025-08-24', num_rows),\n",
    "        'LINENO': [str(random.randint(1,5)) for _ in range(num_rows)],\n",
    "        'VENDOR': [f'VENDOR{random.randint(1,5)}' for _ in range(num_rows)],\n",
    "        'SFC_STAGE': [random.choice(stages) for _ in range(num_rows)],\n",
    "        'STAGEDESCRIPTION': [random.choice(stages) for _ in range(num_rows)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72ae9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Main Unit Assembly Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USN</th>\n",
       "      <th>LINE</th>\n",
       "      <th>WORKSTATION</th>\n",
       "      <th>STAGE</th>\n",
       "      <th>TRNDATA</th>\n",
       "      <th>TRNDATE</th>\n",
       "      <th>PASSCOUNT</th>\n",
       "      <th>RESULTFLAG</th>\n",
       "      <th>USERID</th>\n",
       "      <th>INSERTTIME</th>\n",
       "      <th>LINENO</th>\n",
       "      <th>VENDOR</th>\n",
       "      <th>A_ERRORCODE</th>\n",
       "      <th>AQM_ERRORCODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USN000001</td>\n",
       "      <td>LINE005</td>\n",
       "      <td>WORKSTATION003</td>\n",
       "      <td>M5</td>\n",
       "      <td>USN000927</td>\n",
       "      <td>25-08-24 13:19:58</td>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>USER020</td>\n",
       "      <td>25-08-24 21:48:02</td>\n",
       "      <td>4</td>\n",
       "      <td>VENDOR3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USN000002</td>\n",
       "      <td>LINE002</td>\n",
       "      <td>WORKSTATION001</td>\n",
       "      <td>M1</td>\n",
       "      <td>USN000565</td>\n",
       "      <td>25-08-25 17:59:05</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>USER013</td>\n",
       "      <td>25-08-25 00:36:06</td>\n",
       "      <td>5</td>\n",
       "      <td>VENDOR1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USN000003</td>\n",
       "      <td>LINE002</td>\n",
       "      <td>WORKSTATION001</td>\n",
       "      <td>M4</td>\n",
       "      <td>USN000310</td>\n",
       "      <td>25-08-26 01:09:42</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>USER006</td>\n",
       "      <td>25-08-26 21:43:39</td>\n",
       "      <td>2</td>\n",
       "      <td>VENDOR5</td>\n",
       "      <td>K6611815</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USN000004</td>\n",
       "      <td>LINE005</td>\n",
       "      <td>WORKSTATION009</td>\n",
       "      <td>M2</td>\n",
       "      <td>USN000615</td>\n",
       "      <td>25-08-27 15:40:42</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>USER005</td>\n",
       "      <td>25-08-27 06:07:24</td>\n",
       "      <td>1</td>\n",
       "      <td>VENDOR1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USN000005</td>\n",
       "      <td>LINE005</td>\n",
       "      <td>WORKSTATION001</td>\n",
       "      <td>M1</td>\n",
       "      <td>USN000892</td>\n",
       "      <td>25-08-28 06:28:05</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>USER014</td>\n",
       "      <td>25-08-28 12:40:34</td>\n",
       "      <td>4</td>\n",
       "      <td>VENDOR1</td>\n",
       "      <td></td>\n",
       "      <td>ERROR3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USN     LINE     WORKSTATION STAGE    TRNDATA            TRNDATE  \\\n",
       "0  USN000001  LINE005  WORKSTATION003    M5  USN000927  25-08-24 13:19:58   \n",
       "1  USN000002  LINE002  WORKSTATION001    M1  USN000565  25-08-25 17:59:05   \n",
       "2  USN000003  LINE002  WORKSTATION001    M4  USN000310  25-08-26 01:09:42   \n",
       "3  USN000004  LINE005  WORKSTATION009    M2  USN000615  25-08-27 15:40:42   \n",
       "4  USN000005  LINE005  WORKSTATION001    M1  USN000892  25-08-28 06:28:05   \n",
       "\n",
       "   PASSCOUNT RESULTFLAG   USERID         INSERTTIME LINENO   VENDOR  \\\n",
       "0          1          T  USER020  25-08-24 21:48:02      4  VENDOR3   \n",
       "1          0          F  USER013  25-08-25 00:36:06      5  VENDOR1   \n",
       "2          0          T  USER006  25-08-26 21:43:39      2  VENDOR5   \n",
       "3          2          T  USER005  25-08-27 06:07:24      1  VENDOR1   \n",
       "4          1          F  USER014  25-08-28 12:40:34      4  VENDOR1   \n",
       "\n",
       "  A_ERRORCODE AQM_ERRORCODE  \n",
       "0                            \n",
       "1                            \n",
       "2    K6611815                \n",
       "3                            \n",
       "4                    ERROR3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of Component Assembly Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USN_PRIMARY</th>\n",
       "      <th>SUB_USN</th>\n",
       "      <th>LINE</th>\n",
       "      <th>WORKSTATION</th>\n",
       "      <th>STAGE</th>\n",
       "      <th>TRNDATA</th>\n",
       "      <th>TRNDATE</th>\n",
       "      <th>PASSCOUNT</th>\n",
       "      <th>RESULTFLAG</th>\n",
       "      <th>USERID</th>\n",
       "      <th>INSERTTIME</th>\n",
       "      <th>LINENO</th>\n",
       "      <th>VENDOR</th>\n",
       "      <th>SFC_STAGE</th>\n",
       "      <th>STAGEDESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USN000001</td>\n",
       "      <td>CSN090841</td>\n",
       "      <td>LINE005</td>\n",
       "      <td>WORKSTATION003</td>\n",
       "      <td>C5</td>\n",
       "      <td>CSN000376</td>\n",
       "      <td>25-08-24 07:06:31</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>USER019</td>\n",
       "      <td>25-08-24 03:54:42</td>\n",
       "      <td>1</td>\n",
       "      <td>VENDOR4</td>\n",
       "      <td>ISO to HSG</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USN000002</td>\n",
       "      <td>CSN053121</td>\n",
       "      <td>LINE002</td>\n",
       "      <td>WORKSTATION004</td>\n",
       "      <td>C3</td>\n",
       "      <td>CSN000341</td>\n",
       "      <td>25-08-25 04:10:13</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>USER015</td>\n",
       "      <td>25-08-25 18:54:03</td>\n",
       "      <td>5</td>\n",
       "      <td>VENDOR5</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USN000003</td>\n",
       "      <td>CSN377439</td>\n",
       "      <td>LINE001</td>\n",
       "      <td>WORKSTATION005</td>\n",
       "      <td>C3</td>\n",
       "      <td>CSN000137</td>\n",
       "      <td>25-08-26 11:51:21</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>USER006</td>\n",
       "      <td>25-08-26 04:10:13</td>\n",
       "      <td>1</td>\n",
       "      <td>VENDOR2</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "      <td>BTN Conn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USN000004</td>\n",
       "      <td>CSN231429</td>\n",
       "      <td>LINE004</td>\n",
       "      <td>WORKSTATION008</td>\n",
       "      <td>C3</td>\n",
       "      <td>CSN000515</td>\n",
       "      <td>25-08-27 01:20:40</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>USER013</td>\n",
       "      <td>25-08-27 18:48:22</td>\n",
       "      <td>5</td>\n",
       "      <td>VENDOR3</td>\n",
       "      <td>ISO to HSG</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USN000005</td>\n",
       "      <td>CSN546300</td>\n",
       "      <td>LINE005</td>\n",
       "      <td>WORKSTATION004</td>\n",
       "      <td>C1</td>\n",
       "      <td>CSN000017</td>\n",
       "      <td>25-08-28 11:10:09</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>USER003</td>\n",
       "      <td>25-08-28 11:20:34</td>\n",
       "      <td>3</td>\n",
       "      <td>VENDOR4</td>\n",
       "      <td>DK:Paste MLB tape and ety</td>\n",
       "      <td>BTL:Double Lock Sim Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  USN_PRIMARY    SUB_USN     LINE     WORKSTATION STAGE    TRNDATA  \\\n",
       "0   USN000001  CSN090841  LINE005  WORKSTATION003    C5  CSN000376   \n",
       "1   USN000002  CSN053121  LINE002  WORKSTATION004    C3  CSN000341   \n",
       "2   USN000003  CSN377439  LINE001  WORKSTATION005    C3  CSN000137   \n",
       "3   USN000004  CSN231429  LINE004  WORKSTATION008    C3  CSN000515   \n",
       "4   USN000005  CSN546300  LINE005  WORKSTATION004    C1  CSN000017   \n",
       "\n",
       "             TRNDATE  PASSCOUNT RESULTFLAG   USERID         INSERTTIME LINENO  \\\n",
       "0  25-08-24 07:06:31          2          F  USER019  25-08-24 03:54:42      1   \n",
       "1  25-08-25 04:10:13          3          T  USER015  25-08-25 18:54:03      5   \n",
       "2  25-08-26 11:51:21          2          T  USER006  25-08-26 04:10:13      1   \n",
       "3  25-08-27 01:20:40          3          T  USER013  25-08-27 18:48:22      5   \n",
       "4  25-08-28 11:10:09          0          T  USER003  25-08-28 11:20:34      3   \n",
       "\n",
       "    VENDOR                  SFC_STAGE          STAGEDESCRIPTION  \n",
       "0  VENDOR4                 ISO to HSG  BTL:Double Lock Sim Data  \n",
       "1  VENDOR5   BTL:Double Lock Sim Data  BTL:Double Lock Sim Data  \n",
       "2  VENDOR2   BTL:Double Lock Sim Data                  BTN Conn  \n",
       "3  VENDOR3                 ISO to HSG  BTL:Double Lock Sim Data  \n",
       "4  VENDOR4  DK:Paste MLB tape and ety  BTL:Double Lock Sim Data  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample datasets\n",
    "main_unit_df = generate_main_unit_data(1000)\n",
    "component_df = generate_component_data(1000)\n",
    "\n",
    "# Save to CSV\n",
    "main_unit_df.to_csv('main_unit_assembly_data.csv', index=False)\n",
    "component_df.to_csv('component_assembly_data.csv', index=False)\n",
    "\n",
    "print(\"Sample of Main Unit Assembly Data:\")\n",
    "display(main_unit_df.head())\n",
    "\n",
    "print(\"\\nSample of Component Assembly Data:\")\n",
    "display(component_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76801f40",
   "metadata": {},
   "source": [
    "# Phase 1: Data Preprocessing and Feature Engineering\n",
    "\n",
    "Let's prepare our data for ML models by:\n",
    "1. Cleaning and handling missing values\n",
    "2. Feature encoding\n",
    "3. Creating temporal features\n",
    "4. Scaling/normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dcd33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed!\n",
      "Main unit training set shape: (800, 8)\n",
      "Component training set shape: (800, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_unit_df['A_ERRORCODE'].fillna('NONE', inplace=True)\n",
      "C:\\Users\\srava\\AppData\\Local\\Temp\\ipykernel_20460\\1512934045.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_unit_df['AQM_ERRORCODE'].fillna('NONE', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "main_unit_df = pd.read_csv('main_unit_assembly_data.csv')\n",
    "component_df = pd.read_csv('component_assembly_data.csv')\n",
    "\n",
    "# Convert datetime columns\n",
    "for df in [main_unit_df, component_df]:\n",
    "    for col in ['TRNDATE', 'INSERTTIME']:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        # Extract temporal features\n",
    "        df[f'{col}_hour'] = df[col].dt.hour\n",
    "        df[f'{col}_day'] = df[col].dt.day\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "\n",
    "# Handle missing values\n",
    "main_unit_df['A_ERRORCODE'].fillna('NONE', inplace=True)\n",
    "main_unit_df['AQM_ERRORCODE'].fillna('NONE', inplace=True)\n",
    "\n",
    "# Label Encoding for categorical variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['LINE', 'WORKSTATION', 'STAGE', 'VENDOR']\n",
    "\n",
    "for df in [main_unit_df, component_df]:\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "\n",
    "# Create feature matrices\n",
    "X_main = main_unit_df[[col for col in main_unit_df.columns if '_encoded' in col or '_hour' in col or '_day' in col]]\n",
    "y_main = (main_unit_df['RESULTFLAG'] == 'F').astype(int)  # Convert to binary (0 = Pass, 1 = Fail)\n",
    "\n",
    "X_component = component_df[[col for col in component_df.columns if '_encoded' in col or '_hour' in col or '_day' in col]]\n",
    "y_component = (component_df['RESULTFLAG'] == 'F').astype(int)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_main_scaled = scaler.fit_transform(X_main)\n",
    "X_component_scaled = scaler.fit_transform(X_component)\n",
    "\n",
    "# Split data\n",
    "X_main_train, X_main_test, y_main_train, y_main_test = train_test_split(\n",
    "    X_main_scaled, y_main, test_size=0.2, random_state=42\n",
    ")\n",
    "X_comp_train, X_comp_test, y_comp_train, y_comp_test = train_test_split(\n",
    "    X_component_scaled, y_component, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Main unit training set shape: {X_main_train.shape}\")\n",
    "print(f\"Component training set shape: {X_comp_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51433c3",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "\n",
    "We'll implement three types of models:\n",
    "1. Random Forest for classification\n",
    "2. XGBoost for classification\n",
    "3. LSTM for temporal pattern detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fa35d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       175\n",
      "           1       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.94      0.52      0.51       200\n",
      "weighted avg       0.89      0.88      0.83       200\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       175\n",
      "           1       0.25      0.04      0.07        25\n",
      "\n",
      "    accuracy                           0.86       200\n",
      "   macro avg       0.56      0.51      0.50       200\n",
      "weighted avg       0.80      0.86      0.82       200\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(X_seq), np.array(y_seq)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Prepare sequences for LSTM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m X_lstm_train, y_lstm_train = \u001b[43mprepare_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_main_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_main_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m X_lstm_test, y_lstm_test = prepare_sequences(X_main_test, y_main_test)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Build LSTM model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprepare_sequences\u001b[39m\u001b[34m(X, y, timesteps)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) - timesteps):\n\u001b[32m     33\u001b[39m     X_seq.append(X[i:(i + timesteps)])\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     y_seq.append(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(X_seq), np.array(y_seq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 10"
     ]
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# 1. Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_main_train, y_main_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_pred = rf_model.predict(X_main_test)\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_main_test, rf_pred))\n",
    "\n",
    "# 2. XGBoost\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_main_train, y_main_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_pred = xgb_model.predict(X_main_test)\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "print(classification_report(y_main_test, xgb_pred))\n",
    "\n",
    "# 3. LSTM (for temporal patterns)\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "def prepare_sequences(X, y, timesteps=5):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_seq.append(X[i:(i + timesteps)])\n",
    "        y_seq.append(y[i + timesteps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "X_lstm_train, y_lstm_train = prepare_sequences(X_main_train, y_main_train)\n",
    "X_lstm_test, y_lstm_test = prepare_sequences(X_main_test, y_main_test)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(X_lstm_train.shape[1], X_lstm_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM\n",
    "history = lstm_model.fit(\n",
    "    X_lstm_train, y_lstm_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_pred = (lstm_model.predict(X_lstm_test) > 0.5).astype(int)\n",
    "print(\"\\nLSTM Performance:\")\n",
    "print(classification_report(y_lstm_test, lstm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61c00b",
   "metadata": {},
   "source": [
    "# Visualization and Dashboard\n",
    "\n",
    "Create interactive visualizations using Plotly for:\n",
    "1. Failure predictions and actual outcomes\n",
    "2. Error code patterns\n",
    "3. Temporal trends\n",
    "4. Performance metrics by vendor/stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. Failure Prediction Visualization\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig = px.imshow(cm, \n",
    "                    labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "                    x=['Pass', 'Fail'],\n",
    "                    y=['Pass', 'Fail'],\n",
    "                    title=f'{model_name} Confusion Matrix')\n",
    "    fig.show()\n",
    "\n",
    "plot_confusion_matrix(y_main_test, rf_pred, \"Random Forest\")\n",
    "plot_confusion_matrix(y_main_test, xgb_pred, \"XGBoost\")\n",
    "\n",
    "# 2. Error Code Patterns\n",
    "error_counts = main_unit_df[main_unit_df['A_ERRORCODE'] != '']['A_ERRORCODE'].value_counts()\n",
    "fig = px.bar(x=error_counts.index, y=error_counts.values,\n",
    "             title='Error Code Distribution',\n",
    "             labels={'x': 'Error Code', 'y': 'Count'})\n",
    "fig.show()\n",
    "\n",
    "# 3. Temporal Trends\n",
    "temporal_failures = main_unit_df.groupby('TRNDATE_hour')['RESULTFLAG'].apply(lambda x: (x == 'F').mean())\n",
    "fig = px.line(x=temporal_failures.index, y=temporal_failures.values,\n",
    "              title='Failure Rate by Hour of Day',\n",
    "              labels={'x': 'Hour', 'y': 'Failure Rate'})\n",
    "fig.show()\n",
    "\n",
    "# 4. Performance by Vendor/Stage\n",
    "vendor_performance = main_unit_df.groupby('VENDOR')['RESULTFLAG'].apply(lambda x: (x == 'F').mean())\n",
    "fig = px.bar(x=vendor_performance.index, y=vendor_performance.values,\n",
    "             title='Failure Rate by Vendor',\n",
    "             labels={'x': 'Vendor', 'y': 'Failure Rate'})\n",
    "fig.show()\n",
    "\n",
    "# Create an interactive dashboard layout\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Error Code Distribution', 'Temporal Failure Patterns',\n",
    "                   'Vendor Performance', 'Stage Performance')\n",
    ")\n",
    "\n",
    "# Add plots to dashboard\n",
    "fig.add_trace(\n",
    "    go.Bar(x=error_counts.index, y=error_counts.values),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=temporal_failures.index, y=temporal_failures.values, mode='lines'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=vendor_performance.index, y=vendor_performance.values),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "stage_performance = main_unit_df.groupby('STAGE')['RESULTFLAG'].apply(lambda x: (x == 'F').mean())\n",
    "fig.add_trace(\n",
    "    go.Bar(x=stage_performance.index, y=stage_performance.values),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=1200, title_text=\"Manufacturing Quality Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd3725",
   "metadata": {},
   "source": [
    "# Failure Analysis by USN, Stage, and Vendor\n",
    "\n",
    "We'll create an analysis to:\n",
    "1. Track failures by USN across different stages\n",
    "2. Identify problematic vendor-stage combinations\n",
    "3. Analyze temporal patterns of failures for specific USNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze failures by USN\n",
    "def analyze_usn_failures(usn, main_df, component_df):\n",
    "    # Main unit failures\n",
    "    main_failures = main_df[main_df['USN'] == usn]\n",
    "    main_failures = main_failures[main_failures['RESULTFLAG'] == 'F']\n",
    "    \n",
    "    # Component failures\n",
    "    comp_failures = component_df[component_df['USN_PRIMARY'] == usn]\n",
    "    comp_failures = comp_failures[comp_failures['RESULTFLAG'] == 'F']\n",
    "    \n",
    "    return main_failures, comp_failures\n",
    "\n",
    "# Function to predict failure probability for a specific USN-Stage-Vendor combination\n",
    "def predict_failure_probability(usn, stage, vendor, time, model, scaler, feature_cols):\n",
    "    # Create feature vector\n",
    "    features = pd.DataFrame({\n",
    "        'STAGE_encoded': [le.transform([stage])[0]],\n",
    "        'VENDOR_encoded': [le.transform([vendor])[0]],\n",
    "        'TRNDATE_hour': [pd.to_datetime(time).hour],\n",
    "        'TRNDATE_day': [pd.to_datetime(time).day],\n",
    "        'TRNDATE_month': [pd.to_datetime(time).month]\n",
    "    })\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features[feature_cols])\n",
    "    \n",
    "    # Predict probability\n",
    "    failure_prob = model.predict_proba(features_scaled)[0][1]\n",
    "    return failure_prob\n",
    "\n",
    "# Example analysis for specific USNs\n",
    "sample_usns = main_unit_df['USN'].unique()[:5]  # Take first 5 USNs for example\n",
    "\n",
    "# Create failure analysis dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Failures by Stage', 'Failures by Vendor',\n",
    "                   'Failure Timeline', 'Stage-Vendor Heatmap')\n",
    ")\n",
    "\n",
    "for usn in sample_usns:\n",
    "    main_failures, comp_failures = analyze_usn_failures(usn, main_unit_df, component_df)\n",
    "    \n",
    "    # 1. Failures by Stage\n",
    "    stage_failures = pd.concat([\n",
    "        main_failures.groupby('STAGE').size(),\n",
    "        comp_failures.groupby('STAGE').size()\n",
    "    ]).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=stage_failures['STAGE'], \n",
    "               y=stage_failures[0],\n",
    "               name=f'USN: {usn}'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Failures by Vendor\n",
    "    vendor_failures = pd.concat([\n",
    "        main_failures.groupby('VENDOR').size(),\n",
    "        comp_failures.groupby('VENDOR').size()\n",
    "    ]).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=vendor_failures['VENDOR'],\n",
    "               y=vendor_failures[0],\n",
    "               name=f'USN: {usn}'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Timeline of Failures\n",
    "    all_failures = pd.concat([\n",
    "        main_failures[['TRNDATE', 'STAGE', 'VENDOR']],\n",
    "        comp_failures[['TRNDATE', 'STAGE', 'VENDOR']]\n",
    "    ])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_failures['TRNDATE'],\n",
    "                  y=[usn] * len(all_failures),\n",
    "                  mode='markers',\n",
    "                  name=f'USN: {usn}'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Stage-Vendor Failure Heatmap\n",
    "stage_vendor_failures = pd.crosstab(\n",
    "    main_unit_df[main_unit_df['RESULTFLAG'] == 'F']['STAGE'],\n",
    "    main_unit_df[main_unit_df['RESULTFLAG'] == 'F']['VENDOR']\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=stage_vendor_failures.values,\n",
    "               x=stage_vendor_failures.columns,\n",
    "               y=stage_vendor_failures.index,\n",
    "               colorscale='Reds'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=1200,\n",
    "                 title_text=\"Failure Analysis by USN, Stage, and Vendor\")\n",
    "fig.show()\n",
    "\n",
    "# Predict failures for specific combinations\n",
    "print(\"\\nFailure Probability Predictions:\")\n",
    "for usn in sample_usns[:2]:  # Take 2 USNs for example\n",
    "    main_failures, _ = analyze_usn_failures(usn, main_unit_df, component_df)\n",
    "    if len(main_failures) > 0:\n",
    "        for _, failure in main_failures.iterrows():\n",
    "            prob = predict_failure_probability(\n",
    "                usn, \n",
    "                failure['STAGE'],\n",
    "                failure['VENDOR'],\n",
    "                failure['TRNDATE'],\n",
    "                rf_model,  # Using Random Forest model\n",
    "                scaler,\n",
    "                [col for col in X_main.columns if '_encoded' in col or '_hour' in col or '_day' in col]\n",
    "            )\n",
    "            print(f\"\\nUSN: {usn}\")\n",
    "            print(f\"Stage: {failure['STAGE']}\")\n",
    "            print(f\"Vendor: {failure['VENDOR']}\")\n",
    "            print(f\"Time: {failure['TRNDATE']}\")\n",
    "            print(f\"Predicted Failure Probability: {prob:.2%}\")\n",
    "            if 'A_ERRORCODE' in failure and failure['A_ERRORCODE']:\n",
    "                print(f\"Error Code: {failure['A_ERRORCODE']}\")\n",
    "            if 'AQM_ERRORCODE' in failure and failure['AQM_ERRORCODE']:\n",
    "                print(f\"AQM Error: {failure['AQM_ERRORCODE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b12c66",
   "metadata": {},
   "source": [
    "# Manufacturing Quality Analysis System - TCS AI Hackathon\n",
    "\n",
    "## Components:\n",
    "1. Real-time Failure Prediction System\n",
    "2. LLM-based Root Cause Analysis\n",
    "3. Interactive Dashboard\n",
    "4. Client Benefits Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Manufacturing Analysis System\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1. Enhanced Failure Analysis with Cost Impact\n",
    "def analyze_manufacturing_metrics(main_df, component_df):\n",
    "    # Calculate key metrics\n",
    "    metrics = {\n",
    "        'total_units': len(main_df['USN'].unique()),\n",
    "        'failed_units': len(main_df[main_df['RESULTFLAG'] == 'F']['USN'].unique()),\n",
    "        'failure_rate': len(main_df[main_df['RESULTFLAG'] == 'F']) / len(main_df),\n",
    "        'unique_error_codes': main_df['A_ERRORCODE'].nunique(),\n",
    "    }\n",
    "    \n",
    "    # Vendor Analysis\n",
    "    vendor_metrics = main_df.groupby('VENDOR').agg({\n",
    "        'RESULTFLAG': lambda x: (x == 'F').mean(),\n",
    "        'USN': 'count'\n",
    "    }).reset_index()\n",
    "    vendor_metrics.columns = ['VENDOR', 'failure_rate', 'total_units']\n",
    "    \n",
    "    # Stage Analysis\n",
    "    stage_metrics = main_df.groupby('STAGE').agg({\n",
    "        'RESULTFLAG': lambda x: (x == 'F').mean(),\n",
    "        'USN': 'count'\n",
    "    }).reset_index()\n",
    "    stage_metrics.columns = ['STAGE', 'failure_rate', 'total_units']\n",
    "    \n",
    "    return metrics, vendor_metrics, stage_metrics\n",
    "\n",
    "# 2. Cost Impact Analysis\n",
    "def calculate_cost_impact(main_df, component_df, cost_per_failure=1000):\n",
    "    vendor_costs = main_df[main_df['RESULTFLAG'] == 'F'].groupby('VENDOR').agg({\n",
    "        'USN': 'count'\n",
    "    }).reset_index()\n",
    "    vendor_costs['cost_impact'] = vendor_costs['USN'] * cost_per_failure\n",
    "    \n",
    "    stage_costs = main_df[main_df['RESULTFLAG'] == 'F'].groupby('STAGE').agg({\n",
    "        'USN': 'count'\n",
    "    }).reset_index()\n",
    "    stage_costs['cost_impact'] = stage_costs['USN'] * cost_per_failure\n",
    "    \n",
    "    return vendor_costs, stage_costs\n",
    "\n",
    "# 3. Time-based Analysis\n",
    "def analyze_temporal_patterns(main_df):\n",
    "    main_df['TRNDATE'] = pd.to_datetime(main_df['TRNDATE'])\n",
    "    hourly_patterns = main_df.groupby([main_df['TRNDATE'].dt.hour, 'VENDOR', 'STAGE']).agg({\n",
    "        'RESULTFLAG': lambda x: (x == 'F').mean()\n",
    "    }).reset_index()\n",
    "    return hourly_patterns\n",
    "\n",
    "# Calculate metrics\n",
    "metrics, vendor_metrics, stage_metrics = analyze_manufacturing_metrics(main_unit_df, component_df)\n",
    "vendor_costs, stage_costs = calculate_cost_impact(main_unit_df, component_df)\n",
    "hourly_patterns = analyze_temporal_patterns(main_unit_df)\n",
    "\n",
    "# Model Performance Metrics\n",
    "y_pred_rf = rf_model.predict(X_main_test)\n",
    "accuracy = accuracy_score(y_main_test, y_pred_rf)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_main_test, y_pred_rf, average='weighted')\n",
    "\n",
    "print(f\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1 Score: {f1:.2%}\")\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Failure Rate by Vendor',\n",
    "        'Failure Rate by Stage',\n",
    "        'Cost Impact by Vendor',\n",
    "        'Hourly Failure Patterns',\n",
    "        'Model Performance',\n",
    "        'Real-time Monitoring'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Vendor Failure Rates\n",
    "fig.add_trace(\n",
    "    go.Bar(x=vendor_metrics['VENDOR'], \n",
    "           y=vendor_metrics['failure_rate'],\n",
    "           name='Vendor Failure Rate'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Stage Failure Rates\n",
    "fig.add_trace(\n",
    "    go.Bar(x=stage_metrics['STAGE'],\n",
    "           y=stage_metrics['failure_rate'],\n",
    "           name='Stage Failure Rate'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Cost Impact\n",
    "fig.add_trace(\n",
    "    go.Bar(x=vendor_costs['VENDOR'],\n",
    "           y=vendor_costs['cost_impact'],\n",
    "           name='Vendor Cost Impact'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Hourly Patterns\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=hourly_patterns['hour'],\n",
    "        y=hourly_patterns['VENDOR'],\n",
    "        z=hourly_patterns['RESULTFLAG'],\n",
    "        colorscale='Reds',\n",
    "        name='Hourly Patterns'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Model Performance\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=accuracy * 100,\n",
    "        title={'text': \"Model Accuracy\"},\n",
    "        gauge={'axis': {'range': [0, 100]}},\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Real-time Monitoring (Last 24 hours simulation)\n",
    "recent_data = main_unit_df.sort_values('TRNDATE').tail(24)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recent_data['TRNDATE'],\n",
    "        y=recent_data['RESULTFLAG'].apply(lambda x: 1 if x == 'F' else 0),\n",
    "        mode='lines+markers',\n",
    "        name='Recent Failures'\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1200, width=1200,\n",
    "                 title_text=\"Manufacturing Quality Analysis Dashboard\")\n",
    "fig.show()\n",
    "\n",
    "# Print key insights for business impact\n",
    "print(\"\\nKey Business Insights:\")\n",
    "print(f\"1. Overall Failure Rate: {metrics['failure_rate']:.2%}\")\n",
    "print(f\"2. Most Problematic Vendor: {vendor_metrics.loc[vendor_metrics['failure_rate'].idxmax(), 'VENDOR']}\")\n",
    "print(f\"3. Most Critical Stage: {stage_metrics.loc[stage_metrics['failure_rate'].idxmax(), 'STAGE']}\")\n",
    "print(f\"4. Total Cost Impact: ${vendor_costs['cost_impact'].sum():,.2f}\")\n",
    "print(f\"5. Model Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Save model and metrics for production deployment\n",
    "import joblib\n",
    "joblib.dump(rf_model, 'manufacturing_quality_model.joblib')\n",
    "joblib.dump(scaler, 'feature_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and RAG Integration for Root Cause Analysis\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize LLM and embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create knowledge base from historical data\n",
    "def create_knowledge_base(main_df, component_df):\n",
    "    knowledge_base = []\n",
    "    \n",
    "    # Extract patterns from historical data\n",
    "    for df in [main_df, component_df]:\n",
    "        failed_cases = df[df['RESULTFLAG'] == 'F']\n",
    "        for _, case in failed_cases.iterrows():\n",
    "            entry = {\n",
    "                'stage': case['STAGE'],\n",
    "                'vendor': case['VENDOR'],\n",
    "                'error_code': case.get('A_ERRORCODE', '') or case.get('AQM_ERRORCODE', ''),\n",
    "                'description': f\"Failure at {case['STAGE']} by {case['VENDOR']} with error {case.get('A_ERRORCODE', '')}\",\n",
    "                'timestamp': case['TRNDATE']\n",
    "            }\n",
    "            knowledge_base.append(entry)\n",
    "    \n",
    "    return knowledge_base\n",
    "\n",
    "# Function for RAG-enhanced root cause analysis\n",
    "def analyze_root_cause(usn, stage, vendor, error_code, knowledge_base):\n",
    "    # Create query embedding\n",
    "    query = f\"Failure at {stage} by {vendor} with error {error_code}\"\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Find similar cases\n",
    "    similar_cases = []\n",
    "    for case in knowledge_base:\n",
    "        case_embedding = embedding_model.encode([case['description']])[0]\n",
    "        similarity = cosine_similarity([query_embedding], [case_embedding])[0][0]\n",
    "        if similarity > 0.7:  # Similarity threshold\n",
    "            similar_cases.append(case)\n",
    "    \n",
    "    # Generate analysis prompt\n",
    "    prompt = f\"\"\"Analyze manufacturing failure:\n",
    "    USN: {usn}\n",
    "    Stage: {stage}\n",
    "    Vendor: {vendor}\n",
    "    Error Code: {error_code}\n",
    "    \n",
    "    Similar historical cases: {len(similar_cases)}\n",
    "    \n",
    "    Provide root cause analysis and recommendations.\"\"\"\n",
    "    \n",
    "    # Generate response using LLM\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=200, num_return_sequences=1)\n",
    "    analysis = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return analysis, similar_cases\n",
    "\n",
    "# Create knowledge base\n",
    "knowledge_base = create_knowledge_base(main_unit_df, component_df)\n",
    "\n",
    "# Example usage for a specific failure case\n",
    "sample_failure = main_unit_df[main_unit_df['RESULTFLAG'] == 'F'].iloc[0]\n",
    "analysis, similar_cases = analyze_root_cause(\n",
    "    sample_failure['USN'],\n",
    "    sample_failure['STAGE'],\n",
    "    sample_failure['VENDOR'],\n",
    "    sample_failure.get('A_ERRORCODE', ''),\n",
    "    knowledge_base\n",
    ")\n",
    "\n",
    "print(\"Root Cause Analysis:\")\n",
    "print(analysis)\n",
    "print(\"\\nSimilar Historical Cases:\")\n",
    "for case in similar_cases[:3]:  # Show top 3 similar cases\n",
    "    print(f\"- {case['description']} ({case['timestamp']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
